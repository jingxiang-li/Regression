%!TEX program = xelatex
%# -*- coding: utf-8 -*-
%!TEX encoding = UTF-8 Unicode

\documentclass[12pt,oneside,a4paper]{article}
\usepackage{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\usepackage[pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{pdfstartview={XYZ null null 1}}
\usepackage{url}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{microtype}

\usepackage{amsmath, amsthm, amssymb, amsfonts}

\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}} 

\usepackage[sc]{mathpazo}
\linespread{1.1}
\usepackage[T1]{fontenc}


\usepackage{graphics}
\usepackage{graphicx}
\usepackage[figure]{hypcap}
\usepackage[hypcap]{caption}
\usepackage{tikz}
%\usepackage{grffile} 
%\usepackage{float} 
\usepackage{pdfpages}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{threeparttable}

%\usepackage[square,numbers,super,comma,sort]{natbib}
%\usepackage[backend=biber, style=nature, sorting=none, isbn=false, url=false, doi=false]{biblatex}
%\addbibresource{ref.bib}
%\usepackage[]{authblk}

\usepackage{verbatim}

\newcommand{\problem}[1]
{
    \clearpage
    \section*{Problem {#1}}
}

\newcommand{\subproblem}[1]
{
    \subsection*{Problem {#1}}
}


\newcommand{\solution}
{
    \vspace{15pt}
    \noindent\ignorespaces\textbf{\large Solution}
}

\usepackage{fancyhdr}
\usepackage{extramarks}
\lhead{\hmwkAuthorName}
\chead{\hmwkTitle}
\rhead{\firstxmark}
\cfoot{\thepage}

\newcommand{\hmwkTitle}{STAT 8051 HW 2}
\newcommand{\hmwkAuthorName}{Jingxiang Li}

\setlength\headheight{15pt}
\setlength\parindent{0pt}
\setlength{\parskip}{0.5em}

\newcommand{\m}[1]{\texttt{{#1}}}


\pagestyle{fancy}

\title{\hmwkTitle}
\author{\hmwkAuthorName}
\date{\today}


\begin{document}

\maketitle

<<setup, include=FALSE, cache=FALSE>>=
# set global chunk options
opts_chunk$set(fig.path='figure/', fig.align='center', fig.show='hold',tidy.opts=list(keep.blank.line=FALSE, width.cutoff=60), warning=FALSE, error=FALSE, message=FALSE, echo=TRUE)
#, sanitize=TRUE, dev="tikz")
options(replace.assign=TRUE,width=90)
knit_theme$set("edit-eclipse")
knit_hooks$set(document = function(x) { 
  sub('\\usepackage[]{color}', '\\usepackage[]{xcolor}', x, fixed = TRUE) 
}) 
@

\problem{3.2}
\textbf{Added-variable plots} (Data file: \texttt{UN11}) This problem uses the United Nations example in Section 3.1 to demonstrate many of the properties of added-variable plots. This problem is based on the mean function \texttt{fertility $\sim$ log(ppgdp) + pctUrban}. There is nothing special about a two-predictor regression mean function, but we are using this case for simplicity.

\subproblem{3.2.1}
Examine the scatterplot matrix for (\texttt{fertility, log(ppgdp), pctUrban}), and comment on the marginal relationships.

\solution
<<p321, echo = TRUE, fig.width=8, fig.height=6, out.width='.9\\linewidth'>>=
require(package = "alr4")

data <- data.frame(UN11$fertility, log(UN11$ppgdp), UN11$pctUrban)
panel.cor <- function(x, y, digits = 2, prefix = "cor = ", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

pairs(data, lower.panel = panel.smooth, upper.panel = panel.cor)
@

This is the Scatterplot Matrix for (\texttt{fertility, log(ppgdp), pctUrban}). First, this graph shows decreasing tendencies of the response over two predictors respectively, suggesting that linear regression of \texttt{fertility} over these two predictors dose make sense. Plus, the correlation coefficients between response and two predictors also illustrate that there exists linear relationship among these variables. However, the bad news is that this graph also illustrates strong collinearity between two predictors, which means that the regression model we are going to establish may not give valid results about any individual predictor.

\subproblem{3.2.2}
Fit the two simple regressions for \m{fertility $\sim$ log(ppgdp)} and for \m{fertility $\sim$ pctUrban}, and verify that the slope coefficients are significantly different from 0 at any conventional level of significance.

\solution
<<p322>>=
m1 <- lm(fertility ~ log(ppgdp), data = UN11)
m2 <- lm(fertility ~ pctUrban, data = UN11)
summary(m1)
summary(m2)
@

m1 is the simple regression model for \m{fertility $\sim$ log(ppgdp)}, m2 is for \m{fertility $\sim$ pctUrban}. To see whether the slope coefficients are zero, we only need refer to the p-values of F-test given by two models. Note that the p-values given by two models are all $<2e-16$, suggesting that both two slope coefficients are significantly different from 0, at any conventional level of significance.

\subproblem{3.2.3}
Obtain the added-variable plots for both predictors. Based on the added-variable plots, is \m{log(ppgdp)} useful after adjusting for \m{pctUrban}, and similarly, is \m{pctUrban }useful after adjusting for \m{log(ppgdp)}? Compute the estimated mean function with both predictors included as regressors, and verify the findings of the added-variable plots.

\solution
<<p323, echo = TRUE, fig.width=8, fig.height=4, out.width='.9\\linewidth'>>=
require(package = "ggplot2")
require(package = "gridExtra")

e_f2lp <- m1$residuals
e_f2pU <- m2$residuals
e_lp2pU <- lm(log(ppgdp) ~ pctUrban, data = UN11)$residuals
e_pU2lp <- lm(pctUrban ~ log(ppgdp), data = UN11)$residuals

p1 <- ggplot(data.frame(x = e_pU2lp, y = e_f2lp), aes(x = x, y = y))
p1 <- p1 + geom_point()
p1 <- p1 + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
p1 <- p1 + theme_bw() + theme(text = element_text(size = 14))
p1 <- p1 + ggtitle("pctUrban after log(ppgdp)")

p2 <- ggplot(data.frame(x = e_lp2pU, y = e_f2pU), aes(x = x, y = y))
p2 <- p2 + geom_point()
p2 <- p2 + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
p2 <- p2 + theme_bw() + theme(text = element_text(size = 14))
p2 <- p2 + ggtitle("log(ppgdp) after pctUrban")
grid.arrange(p1, p2, ncol=2)
@

Graph on the left side is the added-variable plot for \m{pctUrban} after \m{log(ppgdp)}, similarly, the one on the right side is the plot for \m{log(ppgdp)} after \m{pctUrban}. From these two graphs we see that \m{log(ppgdp)} is still useful to explain the variance of response after adjusting for \m{pctUrban}. However, it seems that after adjusting for \m{log(ppgdp)}, \m{pctUrban} loses its power of explaining the response term, suggesting that the coefficient of \m{pctUrban} in the two predictor regression model should be insignificant. Let's see.

<<p3231>>=
summary(lm(fertility ~ log(ppgdp) + pctUrban, data = UN11))
@

Note that in this two predictor regression model, as what we expected, the p-value of \m{log(ppgdp)} is negligible, which means \m{log(ppgdp)} is significant in this model. On the other side, the p-value of \m{pctUrban} is $0.918$! which strongly suggests that \m{pctUrban} is not significant in this two predictor model.

\subproblem{3.2.4}
Show that the estimated coefficient for \m{log(ppgdp)} is the same as the estimated slope in the added-variable plot for \m{log(ppgdp)} after \m{pctUrban}. This correctly suggests that all the estimates in a multiple linear regression model are adjusted for all the other regressors in the mean function.

\solution
<<p324>>=
m1 <- coef(lm(fertility ~ log(ppgdp) + pctUrban, data = UN11))
m2 <- coef(lm(e_f2pU ~ e_lp2pU))
m1
m2
@

In the code chunk above, \m{m1} is the two predictor model and \m{m2} the added-variable model for \m{log(ppgdp)} after \m{pctUrban}. We see that the coefficient value of \m{log(ppgdp)} in two predictor model is the same as the slope coefficient in the added-variable model, suggesting that all the estimates in a multiple linear regression model are adjusted for all the other regressors in the mean function.

\subproblem{3.2.5}
Show that the residuals in the added-variable plot are identical to the residuals from the mean function with both predictors.

\solution
<<p325, fig.width=6, fig.height=4, out.width='.8\\linewidth'>>=
m1 <- lm(fertility ~ log(ppgdp) + pctUrban, data = UN11)
m2 <- lm(e_f2pU ~ e_lp2pU)
p <- ggplot(data.frame(m1$residuals, m2$residuals), 
            aes(x = m1$residuals, y = m2$residuals))
p <- p + geom_point()
p <- p + theme_bw() + theme(text = element_text(size = 12))
p <- p + ggtitle("Residuals VS Residuals")
p <- p + xlab("Residuals from added-variable model") +
    ylab("Residuals from two predictor model")
p
@

This graph is means to show that residuals from two models are the same. the x axis is the "residuals from added-variable model", the y axis is the "residuals from two predictor model". It shows that all points, representing residual pairs from two models for each case in this data set, are precisely on the line $y = x$, suggesting that the residuals in the added-variable plot are identical to the residuals from the mean function with both predictors. 

\subproblem{3.2.6}
Show that the t-test for the coefficient for \m{log(ppgdp)} is not quite the same from the added-variable plot and from the regression with both regressors, and explain why they are slightly different.

\solution
<<p326>>=
summary(m1)
summary(m2)
@

In the code chunk above, \m{m1} is the two predictor model and \m{m2} the added-variable model for \m{log(ppgdp)} after \m{pctUrban}. It's easy to see that two t-values are different, suggesting that the t-test for the coefficient for \m{log(ppgdp)} is not quite the same from the added-variable plot and from the regression with both regressors. But it's only a matter of degree of freedom. In \m{m1}, the degree of freedom is $n - 3$, but in \m{m2}, the degree of freedom is $n - 2$. In fact, after adjusting the degree of freedom, the t-test of these two models are essentially the same. 

\problem{4.2}
(Data file: \m{Transact}) The data in this example consists of a sample of branches of a large Australian bank (Cunningham and Heathcote, 1989). Each branch makes transactions of two types, and for each of the branches we have recorded the number \m{t1} of type 1 transactions and the number \m{t2} of type 2 transactions. The response is \m{time}, the total minutes of labor used by the branch. Define $a = (\m{t2} + \m{t1})/2$ to be the average transaction time, and $d = \m{t1} - \m{t2}$, and fit the following four mean functions
$$\begin{array}{lll}
    M1: E(time |t1, t2 ) &=& \beta_{0,1} + \beta_{1,1}t1 + \beta_{2,1}t2\\
    M2: E(time |t1, t2 ) &=& \beta_{0,2} + \beta_{3,2}a + \beta_{4,2}d\\
    M3: E(time |t1, t2 ) &=& \beta_{0,3} + \beta_{2,3}t2 + \beta_{4,3}d\\
    M4: E(time |t1, t2 ) &=& \beta_{0,4} + \beta_{1,4}t1 + \beta_{2,4}t2 + \beta_{3,4}a + \beta_{4,4}d
\end{array}$$

\solution
<<p42>>=
Transact$a <- (Transact$t1 + Transact$t2) / 2
Transact$d <- (Transact$t1 - Transact$t2)
m1 <- lm(time ~ t1 + t2, data = Transact)
m2 <- lm(time ~ a + d, data = Transact)
m3 <- lm(time ~ t2 + d, data = Transact)
m4 <- lm(time ~ t1 + t2 + a + d, data = Transact)
summary(m1)
summary(m2)
summary(m3)
summary(m4)
@

\subproblem{4.2.1}
In the fit of M4, some of the coefficients estimates are labeled as “aliased” or else they are simply omitted. Explain what this means and why this happens.

\solution

It's easy to prove that 
$$\begin{pmatrix}
  \m{a} \\
  \m{d} 
 \end{pmatrix} = 
 \begin{pmatrix}
  \frac{1}{2} & \frac{1}{2}\\
  1 & -1  
 \end{pmatrix}
 \begin{pmatrix}
  \m{t1}\\
  \m{t2} 
 \end{pmatrix}
$$
Hence there exists perfect multicollinearity among these four predictors, suggesting that linear regression model with these four predictors (\m{m4}) is not identifiable. Mathematically, the truth is that $rank(X'X) = 3 < 5$, which means it does not have a inverse matrix.

\subproblem{4.2.2}
What aspects of the fitted regressions are the same? What aspects are different?

\solution

\begin{itemize}
    \item What aspects of the fitted regressions are the same
    \begin{enumerate}
        \item Residuals
        \item Coefficient of determination ($R^2$)
        \item Degree of freedom
        \item Adjusted $R^2$
        \item F-test
        \item Number of valid Predictors
        \item Intercept term
    \end{enumerate}
    \item What aspects are different
    \begin{enumerate}
        \item Coefficients of Predictors
        \item t-test of predictors' coefficients       
    \end{enumerate}
\end{itemize}

\subproblem{4.2.3}
Why is the estimate for \m{t2} different in M1 and M3?

\solution

Since the correlation between \m{t2}, \m{t1} and the correlation between \m{t2}, \m{d} are different. As long as predictors are correlated, interpretation of the effect of a predictor depends not only on the other predictors in a model but also upon which linear transformation of those variables is used.

\problem{4.10}
Suppose you are given random variables $x$ and $y$ such that
$$x \sim N(\mu_{x}, \sigma_{x}^{2})$$
$$y|x \sim N(\beta_{0} + \beta_{1}x, \sigma^{2})$$
so you have the marginal distribution of $x$ and the conditional distribution of $y$ given $x$. The joint distribution of $(x, y)$ is bivariate normal. Find the 5 parameters $(\mu_{x}, \mu_{y}, \sigma_{x}^{2}, \sigma_{y}^{2}, \rho_{x,y})$ of the bivariate normal.

\solution

Given $\mu_{x}$, $\sigma_{x}^2$, $\beta_{0}$, $\beta_{1}$, $\sigma^2$, and given the joint distribution of $(x,~y)$ is bivariate normal, we have 
$$\Bigg\{ \begin{array}{lll}
\beta_{0} &=& \mu_{y} - \rho\sigma_{x}\sigma_{y}\frac{1}{\sigma{x}^2} \mu_{x}\\
\beta_{1} &=& \rho \sigma_{x} \sigma_{y} \frac{1}{\sigma_{x}^2}\\
\sigma^{2} &=& \sigma_{y}^2 - \rho\sigma_{x}\sigma_{y}\frac{1}{\sigma{x}^2}\rho\sigma_{x}\sigma_{y}
\end{array}
$$
$$
\Rightarrow \Bigg\{ \begin{array}{lll}
\beta_{0} &=& \mu_{y} - \rho\frac{\sigma_{y}}{\sigma_{x}}\mu_{x}\\
\beta_{1} &=& \rho \frac{\sigma_{y}}{\sigma_{x}}\\
\sigma^{2} &=& (1 - \rho^2)\sigma_{y}^2
\end{array}
$$
$$
\Rightarrow \Bigg\{ \begin{array}{lll}
\mu_{y} = \beta_{1}\mu_{x} + \beta_{0}\\
\sigma_{y}^2 = \sigma^2 + \beta_{1}^2\sigma_{x}^2\\
\rho = \sqrt{\frac{\beta_{1}^2 \sigma_{x}^2}{\sigma^2 + \beta_{1}^2 \sigma_{x}^2}}
\end{array}
$$

\problem{4.12}
This problem is for you to see what two-dimensional plots of data will look like when the data are sampled from a variety of distributions. For this problem you will need a computer program that allows you to generate random numbers from given distributions. In each of the cases below, set the number of observations $n = 300$, and draw the indicated graphs. Few programs have easy-to-use functions to generate bivariate random numbers, so in this problem you will generate first the predictor $X$, then the response $Y$ given $X$.

\subproblem{4.12.1}
Generate $X$ and $e$ to be independent standard normal random vectors of length $n$. Compute $Y = 2 + 3X + \sigma e$, where in this problem we take $\sigma = 1$. Draw the scatterplot of $Y$ versus $X$, add the true regression line $Y = 2 + 3X$, and the ols regression line. Verify that the scatter of points is approximately elliptical, and the regression line is similar to, but not exactly the same as, the major axis of the ellipse.

\solution
<<p4121, echo = TRUE, fig.width=6, fig.height=4, out.width='.8\\linewidth'>>=
n <- 300
set.seed(123)
x <- rnorm(n = n)
sigma <- 1
y <- 2 + 3 * x + rnorm(n = n, sd = sigma)
f412 = function(x, y)
{
p <- ggplot(data.frame(x, y), aes(x = x, y = y))
p <- p + geom_point()
p <- p + geom_abline(intercept = 2, slope = 3, linetype = 2, size = 1, col = "#ff0000")
p <- p + geom_smooth(formula = y ~ x, method = "lm", se = FALSE, size = 1)
p <- p + theme_bw() + theme(text = element_text(size = 12))
return(p)
}
f412(x, y) + ggtitle("scatterplot of Y versus X")
@

It's easy to see that the scatter of points is approximately elliptical. The solid line is the true regression line, the same as major axis of the ellipse, and the dashed line is the fitted regression line. Note that the fitted regression line is similar to, but not exactly the same as, the major axis of the ellipse.

\subproblem{4.12.2}
Repeat Problem 4.12.1 twice, first set $\sigma = 3$ and then repeat again with $\sigma = 6$. How does the scatter of points change as $\sigma$ changes?

\solution
<<p4122, echo = TRUE, fig.width=8, fig.height=4, out.width='.9\\linewidth'>>=

p1 <- f412(x, y = 2 + 3 * x + rnorm(n = n, sd = 3))
p1 <- p1 + ggtitle("sigma = 3") + theme(text = element_text(size = 14))
p2 <- f412(x, y = 2 + 3 * x + rnorm(n = n, sd = 6))
p2 <- p2 + ggtitle("sigma = 6") + theme(text = element_text(size = 14))
grid.arrange(p1, p2, ncol = 2)
@

As the $\sigma$ increase, the conditional variance of y given x also increase.

\subproblem{4.12.3}
Repeat Problem 4.12.1, but this time set $X$ to have a standard normal distribution and $e$ to have a Cauchy distribution (set $\sigma = 1$). The easy way to generate a Cauchy is to generate two vectors $V_{1}$ and $V_{2}$ of standard normal random numbers, and then set $e = V_{1}/V_{2}$. With this setup, the values you generate are not bivariate normal because the Cauchy does not have a population mean or variance.

\solution
<<p4123, echo = TRUE, fig.width=6, fig.height=4, out.width='.8\\linewidth'>>=
e <- rnorm(n = n, sd = 1) / rnorm(n = n, sd = 1)
p3 <- f412(x, y = 2 + 3 * x + e)
p3 <- p3 + ggtitle("Cauchy Error") + theme(text = element_text(size = 12))
p3
@

It seems that the scatter of points is not as elliptical as plot with normal error, and there exits many of extreme outliers.

\problem{5.8}
\textbf{Cake data} (Data file: \m{cakes})

\subproblem{5.8.1}
Fit (5.12) and verify that the significance levels for the quadratic terms and the interaction are all less than 0.005. When fitting polynomials, tests concerning main effects in models that include a quadratic are generally not of much interest.

\solution
<<p581, echo = TRUE>>=
m <- lm(Y ~ X1 + X2 + I(X1^2) + I(X2^2) + I(X1*X2), data = cakes)
summary(m)
@

Note that the significance levels (p-value) for the quadratic terms and the interaction are all less than 0.005.

\subproblem{5.8.2}
The cake experiment was carried out in two blocks of seven observations each. It is possible that the response might differ by block. For example, if the blocks were different days, then differences in air temperature or humidity when the cakes were mixed might have some effect on $Y$. We can allow for block effects by adding a factor for block to the mean function and possibly allowing for block by regressor interactions. Add block effects to the mean function fit in Section 5.3.1 and summarize results. The blocking is indicated by the variable \m{Block} in the data file.

\solution
<<p582, echo = TRUE>>=
cakes$block <- as.numeric(as.character(cakes$block))
m1 <- lm(Y ~ X1 + X2 + I(X1^2) + I(X2^2) + I(X1*X2) + 
             block + I(X1 * block) + I(X2 * block) + 
             I(X1^2 * block) + I(X2^2 * block) + 
             I(X1*X2 * block), data = cakes)
m2 <- step(object = m1, direction = "both", trace = FALSE)
summary(m2)
@

In this problem we fit regression model with regressors in problem 5.8.2, the main effect of \m{block} and interaction terms between \m{block} and all other predictors. We apply stepwise algorithm in this regression model to make sure that all effects contained in this model are of highly significance.

Note that the main effect of \m{block} is negligible, given other variable fixed. However, the interaction effect of \m{block} and \m{X2}, and \m{block} and $\m{X1}^2$ are significant, suggesting that the influence of \m{block} over the response is indirect.

\problem{5.14}
(Data file: \m{BGSall}) Refer to the Berkeley Guidance study described in Problem 3.3. Using the data file \m{BGSall}, consider the regression of \m{HT18} on \m{HT9} and the grouping factor \m{Sex}.

\subproblem{5.14.1}
Draw the scatterplot of \m{HT18} versus \m{HT9}, using a different symbol for males and females. Comment on the information in the graph about an appropriate mean function for these data.

\solution

<<p5141, echo = TRUE, fig.width=6, fig.height=4, out.width='.8\\linewidth'>>=
BGSall$Sex <- factor(BGSall$Sex)
p <- ggplot(data = BGSall, aes(x = HT9, y = HT18))
p <- p + geom_point(aes(shape = Sex))
p <- p + theme_bw() + theme(text = element_text(size = 12))
p <- p + geom_smooth(formula = y ~ x, method = "lm", se = FALSE, size = 1, aes(colour = Sex))
p <- p + ggtitle("Scatterplot for Different Sex")
p
@

Note that the two slope coefficients of two mean functions are almost the same, but the intercepts are different, suggesting that given the same \m{HT9}, there exists significant difference of \m{HT18} for two levels of \m{Sex}.

\subproblem{5.14.2}
Obtain the appropriate test for a parallel regression model.

\solution

<<p5142>>=
m <- lm(HT18 ~ HT9 + Sex, data = BGSall)
summary(m)
@

We can add the group factor \m{Sex} into the original regression model, and see the t-test result of the coefficient of \m{Sex}. As the result, we see that the significance level of \m{Sex} is very low, suggesting that it should be a parallel regression model.

\subproblem{5.14.3}
Assuming the parallel regression model is adequate, estimate a 95\% confidence interval for the difference between males and females. For the parallel regression model, this is the difference in the intercepts of the two groups.

\solution
<<p5143>>=
confint(object = m, level = .95)
@

To estimate a 95\% confidence interval for the difference between males and females, we only need obtain the 95\% confidence interval for the coefficient of \m{Sex}. As we can see, the confidence interval is $[-12.8635,~ -10.528$]
\end{document}


